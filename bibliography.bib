% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@misc{russo2023actievalita2023overview,
  title         = {ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task},
  author        = {Giuseppe Russo and Niklas Stoehr and Manoel Horta Ribeiro},
  year          = {2023},
  eprint        = {2307.06954},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2307.06954}
}

@inproceedings{de-vries-nissim-2021-good,
  title     = {As Good as New. How to Successfully Recycle {E}nglish {GPT}-2 to Make Models for Other Languages},
  author    = {de Vries, Wietse  and
               Nissim, Malvina},
  booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.findings-acl.74},
  doi       = {10.18653/v1/2021.findings-acl.74},
  pages     = {836--846}
}

@book{Aho:72,
  author    = {Alfred V. Aho and Jeffrey D. Ullman},
  title     = {The Theory of Parsing, Translation and Compiling},
  year      = {1972},
  volume    = {1},
  publisher = {Prentice-Hall},
  address   = {Englewood Cliffs, NJ}
}

@book{APA:83,
  author    = {{American Psychological Association}},
  title     = {Publications Manual},
  year      = {1983},
  publisher = {American Psychological Association},
  address   = {Washington, DC}
}

@article{Chandra:81,
  author  = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
  year    = {1981},
  title   = {Alternation},
  journal = {Journal of the Association for Computing Machinery},
  volume  = {28},
  number  = {1},
  pages   = {114--133},
  doi     = {10.1145/322234.322243}
}

@inproceedings{andrew2007scalable,
  title     = {Scalable training of {L1}-regularized log-linear models},
  author    = {Andrew, Galen and Gao, Jianfeng},
  booktitle = {Proceedings of the 24th International Conference on Machine Learning},
  pages     = {33--40},
  year      = {2007}
}

@book{Gusfield:97,
  author    = {Dan Gusfield},
  title     = {Algorithms on Strings, Trees and Sequences},
  year      = {1997},
  publisher = {Cambridge University Press},
  address   = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
  author  = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
  title   = {Yara Parser: {A} Fast and Accurate Dependency Parser},
  journal = {Computing Research Repository},
  volume  = {arXiv:1503.06733},
  year    = {2015},
  url     = {http://arxiv.org/abs/1503.06733},
  note    = {version 2}
}

@article{Ando2005,
  acmid      = {1194905},
  author     = {Ando, Rie Kubota and Zhang, Tong},
  issn       = {1532-4435},
  issue_date = {12/1/2005},
  journal    = {Journal of Machine Learning Research},
  month      = dec,
  numpages   = {37},
  pages      = {1817--1853},
  publisher  = {JMLR.org},
  title      = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
  volume     = {6},
  year       = {2005}
}

@software{reback2020pandas,
  author    = {The pandas development team},
  title     = {pandas-dev/pandas: Pandas},
  month     = feb,
  year      = 2020,
  publisher = {Zenodo},
  version   = {latest},
  doi       = {10.5281/zenodo.3509134},
  url       = {https://doi.org/10.5281/zenodo.3509134}
}

@inproceedings{devlin-etal-2019-bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  editor    = {Burstein, Jill  and
               Doran, Christy  and
               Solorio, Thamar},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{radford2019language,
  title  = {Language Models are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year   = {2019}
}

@incollection{NEURIPS2019_9015,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{9514855,
  author   = {Guarasci, Raffaele and Minutolo, Aniello and Damiano, Emanuele and De Pietro, Giuseppe and Fujita, Hamido and Esposito, Massimo},
  journal  = {IEEE Access},
  title    = {ELECTRA for Neural Coreference Resolution in Italian},
  year     = {2021},
  volume   = {9},
  number   = {},
  pages    = {115643-115654},
  keywords = {Task analysis;Bit error rate;Natural language processing;Deep learning;Training;Syntactics;Standards;Coreference resolution;ELECTRA;Italian dataset;deep learning;natural language processing},
  doi      = {10.1109/ACCESS.2021.3105278}
}

@article{fi15010015,
  author         = {La Quatra, Moreno and Cagliero, Luca},
  title          = {BART-IT: An Efficient Sequence-to-Sequence Model for Italian Text Summarization},
  journal        = {Future Internet},
  volume         = {15},
  year           = {2023},
  number         = {1},
  article-number = {15},
  url            = {https://www.mdpi.com/1999-5903/15/1/15},
  issn           = {1999-5903},
  abstract       = {The emergence of attention-based architectures has led to significant improvements in the performance of neural sequence-to-sequence models for text summarization. Although these models have proved to be effective in summarizing English-written documents, their portability to other languages is limited thus leaving plenty of room for improvement. In this paper, we present BART-IT, a sequence-to-sequence model, based on the BART architecture that is specifically tailored to the Italian language. The model is pre-trained on a large corpus of Italian-written pieces of text to learn language-specific features and then fine-tuned on several benchmark datasets established for abstractive summarization. The experimental results show that BART-IT outperforms other state-of-the-art models in terms of ROUGE scores in spite of a significantly smaller number of parameters. The use of BART-IT can foster the development of interesting NLP applications for the Italian language. Beyond releasing the model to the research community to foster further research and applications, we also discuss the ethical implications behind the use of abstractive summarization models.},
  doi            = {10.3390/fi15010015}
}

@misc{stefan-it-italian-bertelectra,
  author    = {Stefan Schweter},
  title     = {{Italian BERT and ELECTRA models}},
  version   = {1.0.1},
  publisher = {Zenodo},
  month     = nov,
  year      = 2020,
  doi       = {10.5281/zenodo.4263102},
  url       = {https://zenodo.org/doi/10.5281/zenodo.4263102}
}

@misc{paraschiv2023upbactidetecting,
  title         = {{UPB @ ACTI: Detecting Conspiracies using fine tuned Sentence Transformers}},
  author        = {Andrei Paraschiv and Mihai Dascalu},
  year          = {2023},
  eprint        = {2309.16275},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2309.16275}
}

@inproceedings{lai2023evalita,
  title        = {{EVALITA 2023: Overview of the 8th evaluation campaign of natural language processing and speech tools for italian}},
  author       = {Lai, Mirko and Menini, Stefano and Polignano, Marco and Russo, Valentina and Sprugnoli, Rachele and Venturi, Giulia and others},
  booktitle    = {Proceedings of the Eighth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2023), CEUR. org, Parma, Italy},
  pages        = {3},
  year         = {2024},
  organization = {Accademia University Press},
  url          = {https://ceur-ws.org/Vol-3473/}
}

@inproceedings{hromei2023extremita,
  author    = {Claudiu Daniel Hromei and
               Danilo Croce and
               Valerio Basile and
               Roberto Basili},
  title     = {ExtremITA at EVALITA 2023: Multi-Task Sustainable Scaling to Large Language Models at its Extreme},
  booktitle = {Proceedings of the Eighth Evaluation Campaign of Natural Language
               Processing and Speech Tools for Italian. Final Workshop (EVALITA 2023)},
  publisher = {CEUR.org},
  year      = {2023},
  month     = {September},
  address   = {Parma, Italy},
  url       = {https://ceur-ws.org/Vol-3473/paper13.pdf}
}

@inproceedings{giobergia2023acti,
  author    = {Flavio Giobergia},
  title     = {{Giobergia at Multi-Task Transformer Tuning for Joint Conspiracy Theory Detection and Classification}},
  booktitle = {Proceedings of the Eighth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2023)},
  publisher = {CEUR.org},
  year      = {2023},
  month     = {September},
  url       = {https://ceur-ws.org/Vol-3473/paper13.pdf}
}

@inproceedings{vitali2023acti,
  author    = {Michael Vitali and Vincenzo Scotti and Mark James Carman},
  title     = {{Vitali at ACTI - Transformer-based Conspiracy Theory Identification}},
  booktitle = {Proceedings of the Eighth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2023)},
  publisher = {CEUR.org},
  year      = {2023},
  month     = {September},
  url       = {https://ceur-ws.org/Vol-3473/paper13.pdf}
}

% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{acronym}
\usepackage[inline]{enumitem}
% MUST BE THE LAST IMPORT
\usepackage{cleveref}

\acrodef{acti}[ACTI]{Automatic Conspiracy Theory Identification}
\acrodef{llm}[LLM]{Large Language Model}

\newcommand{\meta}[1]{{\color{blue}#1}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Large Language Models Project \\
Bertinoro International Spring School (BISS) 2024}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Martina Baiardi \and Davide Domini \and Nicolas Farabegoli \and Alessandro Petrella \and Gianni Tumedei }
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\author{
  Martina Baiardi \\
  University of Bologna \\
  {\bf m.baiardi@unibo.it} \\ \And
  Davide Domini \\
  University of Bologna \\
  {\bf davide.domini@unibo.it} \\  \And
  Nicolas Farabegoli \\
  University of Bologna \\
  {\bf nicolas.farabegoli@unibo.it} \\  \AND
  Alessandro Petrella\\
  University of Bologna \\
  {\bf alessandro.petrella@unibo.it} \\ \And
  Gianni Tumedei \\
  University of Bologna \\
  {\bf gianni.tumedei2@unibo.it}
}

\begin{document}

\maketitle

\begin{abstract}
Several social network platforms - such as Telegram, 4chan, and Parler - do not employ strong moderation
policies, causing the proliferation of conspiracy theories in many contexts, like COVID-19 or the war
in Ukraine, and thus diffusing dangerous ideas and generating social harm among their communities.
%
In this paper, we present the process to fine-tune two \acp{llm} architectures
on an Italian dataset from an EVALITA challenge, that requires identifying
conspiracy theories in posts coming from Telegram.
%
We provide details about the two adopted architectures, the model configuration and the training
process adopted in our experiments.
%
\meta{Briefly summarise the finding of the experiments}
\end{abstract}

\section{Task description}\label{sec:task-description}
We conducted a comparative study between \emph{Encoder-based} and \emph{Decoder-only} architectures
on the same task.
%
Specifically, we worked on the \emph{\ac{acti}}~\footnote{\url{https://russogiuseppe.github.io/ACTI/}}
task, that requires identifying conspiracy theories coming from lax moderated policies platforms like
Telegram, 4chan, and Parler.
%
We focused on the \emph{Subtask A}, where a system must recognise if a Telegram post is conspiratorial
or not.
%
To be flagged as conspiratorial, a post must:
\begin{enumerate*}[label=(\roman{*})]
  \item express the belief that major events are controlled by and/or manipulated by powerful people protecting their interests; or
  \item interpret events in a way that supports conspiracy theories.
\end{enumerate*}
A sentence is considered conspiratorial even if it shares some claims intended to undermine commonly accepted views on societal issues.

\section{Dataset description}\label{sec:dataset-description}
The dataset for \ac{acti}-A is in CSV format and contains three columns:
\begin{itemize}
  \item \textbf{id}: represents a unique identifier for the post;
  \item \textbf{comment\_text}: contains the raw text written in the post; and
  \item \textbf{conspiratorial}: represents a binary label where 0 indicates that the post is not conspiratorial,
    while 1 indicates conspiratorial content.
\end{itemize}
The dataset is split into two separate CSV files: one, meant for the training process, lists
1842 records, while the other includes 460 posts for testing purposes.
%
Notably, since the task was part of a challenge, the second dataset does not contain the
\textbf{conspiratorial} column by default, but the \ac{acti} team was kind enough to provide the
missing labels upon our request.

@dom can you add something about the problems initially encountered with test-set.

\section{Architecture overview}\label{sec:architecture-overview}
Provide a minimal description of the neural architecture employed.
@gtumedei

\subsection{Encoder}

\subsection{Decoder}


\section{Experimental setup}\label{sec:experimental-setup}

\subsection{Preprocessing}\label{sec:preprocessing}
Preprocessing is the same for both models, @nicolasfara can describe the process.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figures/class_distribution.pdf}
  \caption{TBD}
  \label{fig:class-frequency}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figures/comment_length_distribution.pdf}
  \caption{
    Comment length distribution for both the training dataset (on the left) and test dataset
    (on the right).
    %
    The upper charts refer to the raw content of posts, coming from the original dataset;
    in the lower charts, our cleanup preprocessing has been applied to the content.
  }
  \label{fig:words-distribution}
\end{figure*}

\subsection{Model configuration}\label{sec:model-config}
tokenizer, parameters, model used \dots @marti    

\paragraph{Encoder.}

\paragraph{Decoder.}


\subsection{Training process}\label{sec:training-process}
epoche, learning rate, ... @dom

\section{Result and analysis}\label{sec:results-analysis}
@ale
\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/decoder-only-confusion-matrix.pdf}
  \caption{
   TBA}
  \label{fig:decoder-only-cm}
\end{figure}


\nocite{*}

\bibliography{bibliography}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
